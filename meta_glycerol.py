#!/usr/bin/python3

'''
Meta file for analysis of glycerol data
from Clark. Reuses generalized fns from
meta.py.

Jordan Dehmel, 2023
jdehmel@outlook.com
'''

import os
import re
import numpy as np
import pandas as pd
from typing import *
from matplotlib import pyplot as plt

import name_fixer
import filterer
import reverser
import time

'''
Sections:

120 vs 240 um
Bottom Up vs Top Down
8, 12, 16, 20 v
SUBSECTION LABELS CHANGE BY SECTION

The files loaded here are ALREADY FILTERED
We just want to perform more analysis on
them.
'''

'''
size	solution	    min freq	max freq	midpoint
5	    1 X 10-4 M	    12	        17	        14.5
5	    5 X 10-5 M	    9	        12	        10.5
10	    1 X 10-4 M	    10	        15	        12.5
10	    5 X 10-5 M	    4	        7	        5.5
'''

file_location: str = '/home/jorb/data_graphs'

# These filters are arranged in hierarchical order
main_folder_filter: str = ''

size_filters: [str] = ['120[_ ]?um', '240[_ ]?um']
directional_filters: [str] = ['[Tt]op[_ ]?[Dd]own', '[Bb]ottom[_ ]?[Uu]p']

voltage_filters: [str] = ['8[_ ][vV]', '12[_ ][vV]', '16[_ ][vV]', '20[_ ][vV]']
z_position_filters: [str] = ['8940', '8960', '8965', '8990', '8980',
                             '8985', '9010', '9015', '9035', '9040',
                             '9060', '9080', '9090', '9115', '9140',
                             '9197', '9255', '9280', '9305', '9240',
                             '9265', '9290', '9315', '9340', '9180',
                             '9205', '9230', '9255', '9280',
                             'top-100', 'top-25', 'top-50', 'top-75', 'top-97', 'top_',
                             'bot+25', 'bot+50', 'bot_']

# This string must be in the filename in order to load it
final_file_qualifier: str = 'track_data_summary.csv'

save_number: int = 0


# Loads a list of lists of filters, and creates permutations of them.
# Returns a list of tuples. Each item is (name, matched_filter_set, data_file, std_file)
def load_all_filter_permutations(hierarchy: [[str]], current_filters: [str] = []) -> List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]]:
    
    if len(hierarchy) == 0:
        # Recursive base case
        name_array: [str] = name_fixer.find_all(current_filters[0])

        for filter in current_filters[1:]:
            temp: [str] = name_fixer.find_all(filter)

            name_array = [name for name in name_array if name in temp]

            if len(name_array) == 0:
                break

        name_array = [name for name in name_array if re.search(final_file_qualifier, name) is not None]

        # Build list of tuples
        out: List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]] = []

        for name in name_array:
            data_file: pd.DataFrame = pd.read_csv(name)
            std_file: pd.DataFrame = pd.read_csv(name.replace('.csv', '_stds.csv'))

            to_append: Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame] = (name, name.replace('.csv', '_stds.csv'), current_filters[:], data_file, std_file)
            out.append(to_append)

        return out

    else:
        out: List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]] = []

        for filter in hierarchy[0]:
            result = load_all_filter_permutations(hierarchy[1:], current_filters + [filter])
            
            for item in result:
                out.append(item)
        
        return out


# Takes a list of filters and a dataset (generated by load_all_filter_permutations)
# and graphs all data which match the given filters on a single graph.
def graph_all_from_filter_list_and_dfs(filters: [str],
                                       data: List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]],
                                       where: [str],
                                       turning_point_lookup: List[Tuple[List[str], float]] = []) -> None:
    global save_number

    # Scrape all items in data which match the specified filters
    items_which_match: List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]] = []
    
    for item in data:
        did_match: bool = True

        for filter in filters:
            if filter not in item[2]:
                did_match = False
                break

        if did_match:
            items_which_match.append(item)

    if len(items_which_match) == 0:
        save_number += 1
        return

    ordered_data: [[float]] = []
    ordered_turning_points: [float] = []
    ordered_frequencies: [StopAsyncIteration] = []
    ordered_errors: [[float]] = []
    ordered_line_labels: [str] = []

    subtitle: str = 'From filter set: ' + str(filters)
    axis_labels: Tuple[str, str] = ('Applied Frequency (Hz)', 'Mean Straight Line Speed (Pixels Per Frame)')
    save_paths: [str] = [path + str(save_number) + '.png' for path in where]
    save_number += 1

    # Build data arrays here for passing into graph_multiple_relative
    for item in items_which_match:
        # Ensure correct sort (prevents weird lines)
        item[3].sort_values('Unnamed: 0', inplace=True)
        item[4].sort_values('Unnamed: 0', inplace=True)

        # item[0] is file name
        ordered_line_labels.append(item[0][60:])

        # item[1] is std name
        # item[2] is filters which found it

        # item[3] is data
        ordered_data.append([row[1]['MEAN_STRAIGHT_LINE_SPEED'] for row in item[3].iterrows()])

        # item[4] is errors
        ordered_errors.append([row[1]['MEAN_STRAIGHT_LINE_SPEED_STD'] for row in item[4].iterrows()])

        # Attempt turning point lookup
        # If cannot be found, default to 12khz5
        match_index: int = -1
        for i, entry in enumerate(turning_point_lookup):
            if entry[0] == item[2]:
                match_index = i
                break

        if match_index != -1:
            ordered_turning_points.append(turning_point_lookup[match_index][1])

        else:
            ordered_turning_points.append('12500.0')

        # Scrape frequencies, not necessarily including turning point
        current_frequencies: [str] = []
        for row in item[3].iterrows():
            freq: float = float(row[1][0])
            current_frequencies.append(str(freq))

        ordered_frequencies.append(current_frequencies[:])

    reverser.graph_multiple_relative(
        data=ordered_data,
        turning_points=ordered_turning_points,
        labels=ordered_frequencies,
        save_paths=save_paths,
        axis_labels=axis_labels,
        line_labels=ordered_line_labels,
        subtitle=subtitle,
        errors=ordered_errors
    )
    
    return


def yield_all_filter_permutations(hierarchy: [[str]], current_filters: [str] = []) -> [str]:
    if len(hierarchy) == 0:
        yield current_filters
        return

    else:
        for filter in hierarchy[0]:
            for item in yield_all_filter_permutations(hierarchy[1:], current_filters + [filter]):
                yield item
                    
        return


if __name__ == '__main__':
    # Build turning point lookup table
    turning_point_lookup: List[Tuple[List[str], float]] = []

    os.chdir(file_location)
    hierarchy: [[str]] = [size_filters, directional_filters, voltage_filters, z_position_filters]

    # Get all the files requested
    print('Loading files...')
    files: List[Tuple[str, str, List[str], pd.DataFrame, pd.DataFrame]] = load_all_filter_permutations(hierarchy)

    print('Loaded', len(files), 'patterns, for a total of', len(files) * 2, 'files.')

    # Graph groups of files matching given filters
    print('Graphing selected files...')

    graph_all_from_filter_list_and_dfs(['120[_ ]?um', '12[_ ][vV]'], files, ['/home/jorb/Programs/physicsScripts/'], turning_point_lookup)

    print('Loading permutations...')
    perm_list = []
    for perm in yield_all_filter_permutations(hierarchy):
        for i in range(1, len(perm) + 1):
            sub_perm = perm[:i]

            if sub_perm not in perm_list:
                print(sub_perm)
                perm_list.append(sub_perm[:])

    print('Graphing', len(perm_list), 'permutations...')

    # Graph every possible graph
    for perm in perm_list:
        print(save_number, 'of', len(perm_list))
        graph_all_from_filter_list_and_dfs(perm, files, ['/home/jorb/Programs/physicsScripts/perm/'], turning_point_lookup)

    # Exit program
    exit(0)
